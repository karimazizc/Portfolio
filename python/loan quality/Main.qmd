---
op---
title: "Money Lion stage 2 project"
author: "Karim Abdul Aziz Chatab"
format: 
  html: 
    code-fold: true
    code-tools: true
    code-summary: "show code"
    toc: true
    theme: cosmo
    code-overflow: wrap
    self-contained: true
editor: visual
execute:
  warning: false
  message: false
  echo: fenced
---

# Predicting Loan Repayment

## 1. Introduction

### 1.1 Business Context

This project was given 3 dataset from data.zip file where I, as the (potential) data scientist at MoneyLion plans to assess the loan repayment quality of the given customer in the dataset.

### 1.2 Required Libraries

```{r}
#----- Run this if not packages arent downloaded, or execute renv
# install.packages("tidyverse")
# install.packages("visdat")
# install.packages('readxl')
# install.packages('tidymodels')

library(tidyverse)
library(tidymodels)
library(readxl)
library(visdat)
library(gt)
library(scales)
library(summarytools)
library(corrplot)
```

### 1.3 Dataset Overview

Full information of the dataset is referred from the dictionary. But in short,

::: callout-note
## loan.csv

This file returns 19 columns consist of the customer's loan information along with their ssn and id. Every row represents and accepted loan application.
:::

::: callout-note
## payment.csv

This file returns 9 columns consist of the customer's payment information along with their ACH error codes and loan id.
:::

::: callout-note
## clarity_underwriting_variables.csv

This file returns 54 rows and 10 columns where each row variable determine the underwriting report.
:::

#### *Importing the dataset:*

```{r}
loan = read_csv('data/loan.csv') 
payment = read_csv('data/payment.csv')
clarity= read_csv('data/clarity_underwriting_variables.csv') 
```

## 2. Data Preprocessing

### 2.1 Exploratory Data Analysis

```{r}
# Loan.csv
loan[0:5,] %>% gt(caption = "Loan.csv first 5 values")
n_loan = nrow(loan)
```

```{r}
# Payment.csv
payment[0:5,] %>% gt(caption = "Payment.csv first 5 values")
n_payment = nrow(payment)
paste("Payment Rows:",comma(n_payment),"Loan Rows:",comma(n_loan))
```

#### 2.1.1 Summary Statistic

##### Frequency table for numerical variables

```{r}
library(summarytools)
descr(loan)
# summary table for loan.csv
approval_rate = sum(loan$approved) /nrow(loan)
```

##### Frequency table for categorical variable

```{r}
# Frequency table for approved
freq(loan$approved)
```

```{r}
# Frequency table for loanstatus
freq(loan$originated)
```

```{r}
# Frequency table for loanstatus
freq(loan$loanStatus)
```

```{r}
# Frequency table for loanstatus
freq(loan$loanStatus)
```

```{r}
freq(loan$leadType)
```

```{r}
freq(loan$fpStatus)
```

```{r}
nPaidOff_mean = paste0(round(mean(drop_na(loan['nPaidOff'])$nPaidOff) *100, digits=2), "%")
paste("Average pay off rate:",nPaidOff_mean)
```

loan.csv shows a high-risk lending operation due to very low approval where 40,036 loan was approved out of 577,682 it has 6.93% approval rate. Although relatively high APR (median of 590%), only 7.96% Originated from applications. High APRs, low approval rates, low originated rates, and low pay off rate suggesting this is a high-risk lending operation indicate majority are **subprime borrower base with significant credit risk.**

```{r}
loan_numeric = loan %>% select(originallyScheduledPaymentAmount, loanAmount, apr)

paymentAmount_max = max(loan_numeric$originallyScheduledPaymentAmount)
loanAmount_max = max(filter(loan_numeric,!is.na(loanAmount))$loanAmount)
loanAmoun_min = min(filter(loan_numeric,!is.na(loanAmount))$loanAmount)

# function for convenient plot
ann = function(x, y, z = "black"){
  annotate('label', x = x, y =y ,label = comma(x, accuracy = 0.01), color = z)
}

vline = function(x, y){geom_vline(xintercept = x, linetype = "dashed", color = y)}

loan_numeric %>% ggplot() + 
  geom_histogram(aes(loanAmount, fill = "Loan Amount"), alpha = 0.5) + 
  geom_histogram(aes(originallyScheduledPaymentAmount, 
                     fill ="Originally Scheduled Payment Amount" ), alpha = 0.5)+ 
  theme_bw()+
  theme(legend.position ='bottom')+
  labs(fill = "Variable: ") +
  xlab("Dollars (USD)") +
  ann(loanAmount_max, y = 50000, z = "salmon") +
  ann(paymentAmount_max, y = 50000, z = "turquoise")+
  vline(loanAmount_max, "salmon")+
  vline(paymentAmount_max, "turquoise")

```

Assuming this is in USD (since all the states mentioned are from United States). With loan amounts generally being higher than scheduled payments. The dashed line indicate the maximum for loan amount and payment amount. Maximum loan is \$5,000 while maximum original payment shows the 4x multiplier from interest and fees resulting in \$19,963.63 payment amount.

```{r}
freq(loan$payFrequency)
```

Majority of 54.94% or 316,654 loans were paid biweekly (B) where least common were paid semi-monthly (S). However, there are mising data in this variable. lets investigate!

```{r}
filter(loan,is.na(payFrequency)) %>% head(5) %>% gt()
```

Apparently, the first 5 `approved` variable is false, lets call all the unique values in payfrequency.

```{r}
unique(filter(loan,is.na(payFrequency))$approved)
```

So it seems all NA from pay frequency is a result of unapproved loan.

##### Summary Statistic of payment.csv

```{r}
# summary table for payment.csv
descr(payment)
freq(payment$isCollection)
freq(payment$paymentReturnCode)
freq(payment$paymentStatus)
```

It seems unusual how, principal, fees and payment amount has a negative value. Although, this could be because of data entry errors, refunds or reversals and system errors in calculation.

```{r}
fees_max = max(payment$fees)
payamount_max = max(payment$paymentAmount)
principal_max = max(payment$principal)

payment %>% ggplot() + 
  geom_histogram(aes(principal, fill = "principal"), alpha= 0.5) +
  geom_histogram(aes(paymentAmount, fill = "paymentAmount"),   alpha= 0.5) +
  ylab(NULL)+
  geom_histogram(aes(fees, fill = "fees"),  alpha= 0.5) +
  labs(fill = "values") + 
  ann(fees_max, 50000, 'salmon')+
  ann(principal_max, 100000, "turquoise")+
  ann(payamount_max, 30000, 'lightblue') + 
  vline(fees_max,'salmon')+
  vline(payamount_max,'turquoise')+
  vline(principal_max,'lightblue')+ theme_bw()
  
  
```

```{r}
payment %>%
  count(loanId) %>% 
  ggplot(aes(x = n)) + 
  geom_histogram() +
  theme_bw() +
  ggtitle("Distribution of total installments per loan")  +
  ann(max(payment$installmentIndex), 5000)+
  vline(max(payment$installmentIndex), "black") + ylab(NULL) +
  ann(median(payment$installmentIndex), 7000) +
  vline(median(payment$installmentIndex),"black") 
print("Summary of installment index");summary(payment$installmentIndex)
```

The maximum installment is 105 installments, but it's rare - the distribution is heavily right-skewed, suggesting most borrowers have shorter-term repayment schedules where most loans have less than 25 installments payments. The average installments per loan is repaid per 10 installment while the median is 9.

```{r}
plot_installments = function(x){
  top_installments = payment %>%
    group_by(loanId) %>% 
    summarise(max_installment = max(installmentIndex)) %>%
    filter(max_installment > x)  
  
  payment %>%
    filter(loanId %in% top_installments$loanId) %>%
    ggplot(aes(paymentDate, fees, color = loanId)) + 
    geom_line(show.legend = FALSE) +
    theme_bw() +
    ggtitle(paste("Installments >=", x))
}

plot_installments(60)
```

The graph tracks fee patterns for loans with 60 or more installments from this dataset. Fees generally decrease over time from \$50-150 initial amounts, but show sharp spikes likely representing late payment penalties, collection charges, or loan restructuring events. Most loans follow similar declining patterns despite varying terms indicating the end of their loan term.

#### 2.1.2 Missing Values

```{r}
# missing values for loan.csv
vis_miss(loan,warn_large_data = FALSE)
```

```{r}
# missing values for payment.csv
vis_miss(payment,warn_large_data = FALSE)
```

Unfortunately 4 columns in the dataset have more missing values than not. Before we explore further, we need to understand the pattern of these NA values. Firstly for payment.csv, majority of the missing values lies on the **paymentReturnCode.**

```{r}
# ACH codes
print_returncode = function(){
cat("ACH codes: \n")
for (x in unique(payment$paymentReturnCode)){
    cat("|",x )
}
}
print_returncode()
```

```{r}
# payment status codes
print_payment = function(){
cat("Payment Status:\n")
for (x in unique(payment$paymentStatus)){
  for (i in 5){
    cat("|",x , "|","\n")
  }
}
}
print_payment()
```

```{r}
# print amount of ACH get
for (x in unique(payment$paymentStatus)){
  p = filter(payment, paymentStatus ==x) %>% 
    filter(!is.na(paymentReturnCode))
  cat(x,":",nrow(p),"\n")
}
```

Note that, payment return code only occurs when the paymentStatus is rejected, checked, cancelled, or returned. Resulting in a code which explains the reject. This is a clear example of MAR (Missing At Random). We can fully explain why the data is missing based on paymentStatus. The same goes to originatedDate and fpStatus. originatedDate only exist if the loan has been originated and fpStatus returns NA if no ACH attempt has been made yet according to the data dictionary.

```{r}
# check for their loan id
unique_loan_payment = nrow(count(payment, loanId))

unique_loan_loan=  nrow(count(loan, loanId))

count(loan,loanId) %>% 
  arrange(desc(n)) %>% 
  head(5)

count(payment,loanId) %>% 
  arrange(desc(n)) %>% 
  head(5)
  
paste("Unique loanId in payment.csv:",comma(unique_loan_payment),"out of",comma(nrow(payment)))
paste("Unique loanId in loan.csv:", comma(unique_loan_loan),"out of",comma(nrow(loan)))
```

payment and loan have different amount of rows, notice that payment's loanID, suggesting that each row does not represent a unique customer. When we try to fit in to the machine learning model, we can't just summarize either, since installments frequency and other ordinal variable might be a useful feature for our prediction. After joining this data set, we need to seperate those rows to be longer where each row represents one loanId. **Due to this we plan to use an encoding methods** later to fit in the model.

Additionally, it is quite strange that the loanId from loan.csv has different order of values, this might not be loanId at all.

```{r}
loan %>% filter(
  loanId =="54b1f646e4b06aa735b27461"
) %>% gt()
```

Turns out this loanId is subject to a withdrawn application. but lets filter all of them that does not follow the same ID structure.

```{r}
uniqueid = loan %>% filter(
    !startsWith(loanId, "LL-I-")
)
uniqueid %>% 
  select(loanId, originated, loanStatus, approved) %>% gt() %>%
  tab_header(
    title = "loanId with different id structure")
```

Oh this makes sense, these id are a result of loanStatus being paid off, withdrawn, pending application, void and external collection.

#### 2.1.3 Joining data

```{r}
df = payment %>%
  left_join(loan, by = "loanId") %>%
  left_join(clarity, 
            by = c("clarityFraudId" = "underwritingid"))
df %>% 
  head(5) %>% 
  gt() %>%
  tab_header(
    title = "Complete Data Frame",
    subtitle = "Joined dataset of loan.csv, payment.csv and clarity_underwriting_variables.csv"
  ) %>%
  tab_style(
    style = cell_text(align = 'left'),
    locations = cells_title()
  ) %>%
  tab_footnote(
    paste0("Total Rows:",comma(nrow(df))," Total Columns:", length(df))
    )
```

Before we can explore further on this dataset, we want to know if there are any missing values.

Additionally, we want to assess whether this join is successful or not

```{r}
# missing loan 
missing_loan <- df %>%
 summarise_all(~mean(is.na(.)) * 100) %>%
 gather() %>%
 arrange(value)

missing_values = df %>% 
  summarise_all(~sum(is.na(.))) %>% 
  gather() %>%
  mutate(not_missing = nrow(df) - value) %>% 
  inner_join(missing_loan, by = "key") %>% rename(
    n_missing = value.x,
    percent_missing = value.y
  ) %>% arrange(desc(percent_missing))

missing_values  %>% filter(percent_missing > 0) %>%
  gt() %>%
  tab_header(title = "All Variables with Missing values","source: df's missing values (joined_data) in descending order") %>%
  fmt_number(
    columns = percent_missing,
    decimals = 5
  ) %>%
  tab_style(
    style = cell_text(align ="center", size = px(12)),
    locations = cells_body()
  )%>%
  tab_style(
    style = cell_text(align ="center"),
    locations = cells_column_labels()
  ) %>% cols_label(
    key = "variable",
    percent_missing = "NA (%)",
    n_missing = "NA amount",
    not_missing = "not NA amount"
  )
```

it seems the initial fraud screening has 19-20% missing values while clear fraud identity verification has missing rates of 95-97% implying this is a result of selective verfication process based on risk assessment needs. Due to this we plan to not include all the variables for the model as the high NA values might add white noise to the model. Although, the clarity underwriting values that we'll prioritize is:

-   clearFraudScore (measure overall fraud risk)

-   totalnumberoffraudindicators (aggregate fraud signals)

-   clearfraudinquiry (inquiries of n-th times ago)

-   ssnnamematch (verify identity)

-   nameaddressmatch (address verification)

-   overallmatchresult (combined verficiation score)

-   creditestablishedbeforeage18 (potential fraud)

-   currentaddressreportedbynewtradeonly (suspicious signal)

-   morethan3inquiriesinthelast30days (shows desperation)

-   inputssninvalid (basic fraud check)

### 2.2 Determining the dependent variable.

```{r}
print_payment()
```

After the exploratory data analysis, **PaymentStatus** would be the ideal dependent variable for predicting loan risk. This because it captures the complete process of loan performance. Payment status have no NA values and updates throughout the loan term. Additionally, paymentstatus reveals the 9 indicators of loan progression from "Checked" to "Complete". This variable aligns perfectly with the business context where it is a measure of the applicant's behaviour and the loan's health.

## 3. Feature Engineering

Our goal here is to reduce dimensionality and create a better predictive power. Additionally, we wanted this to be a white box model. That is because we plan to create a model that can still be interpret rather than being highly accurate. To compensate on predictive power, we want the model to be explainable due to financial regulations prefer our fraud prediction to be transparent for fair lending practices, regulatory compliance, model validation and customer explanation requirements.

### 3.1 Selecting Useful Inputs

For the maching learning model to work, we wanted to delete any redundant variables that will not be useful for prediction. This include, ID , highly correlated variables (principal and fees with payment amount) and dates. We also wanted to remove state as that might introduce regional bias. Although, we plan to keep the loanID to aggregate the columns later.

```{r}
df_feature = df %>%
  select(loanId,
         paymentStatus, # Target  variable
         installmentIndex, # We'll encode this later 
         isCollection,
         principal,
         fees,
         paymentReturnCode,
         payFrequency,
         apr,
         isFunded,
         approved,
         loanAmount,
         originallyScheduledPaymentAmount,
         nPaidOff,
         originated,
         loanStatus,
         leadType,
         fpStatus,
         # Clarity underwriting variables that we'll input
         clearfraudscore,
         contains('clearfraudinquiry'),
         contains('creditestablishedbeforeage18'),
         contains('ssnnamematch'),
         contains('currentaddressreportedbynewtradeonly'),
         contains('morethan3inquiriesinthelast30days'),
         contains('overallmatchresult'),
         contains('totalnumberoffraudindicators'),
         contains('inputssninvalid'),
         contains('nameaddressmatch')
         )

new_names <- c(
  # Extract everything after the last dot
  df_feature %>% 
    select(contains('underwritingdataclarity')) %>% 
    names() %>% 
    setNames(., gsub(".*\\.", "", .))
)

df_feature <- df_feature %>% 
  rename(!!!new_names)

head(df_feature, 5) %>% 
  gt() %>%
  tab_header(
    title ="Selected features table (first 5 rows)",
    subtitle = "Initial selected columns "
  ) %>%
  tab_style(
    style = cell_text(align = 'left'),
    locations = cells_title()
  ) %>%
  tab_footnote(
  paste("column:", length(df_feature),
        "row:", comma(nrow(df_feature))))

```

```{r}
df_cor = na.omit(df_feature) %>% 
  select_if(is.numeric) 

colnames(df_cor) <- substr(colnames(df_cor), 1, 10) 
df_cor %>% cor() %>% 
  corrplot(method = "number",
           addrect = 2)
```

This correlation matrix present correlation challenges for the model. High multicollineaity between time variables might need to be aggregated to prevent redundancy. Creating ratio features would probably be efficient. For the installment index, as discussed previously in data preprocessing stage we plan to implement an encoding methods to aggregate the data.

### 3.2 Missing Values by removal

```{r}
vis_miss(df_feature, warn_large_data = FALSE)
```

If we want to filter the data we need to make sure the missing data is Missing Completely At Random (MCAR) so it wont create selection bias, hoping this missingness doesn't correlate with important outcomes. Additionally the payment return code has 95% missing values, it is better to completely drop them. The fraud indicators likely to be Missing At Random (MAR) or Missing Not At Random (MNAR) as the missing visualization shows consistent pattern. to double check, this data is a result of joining three tables. Hence, let's visualize the clarity fraud id with clearfraudscore on the previous joined dataset. However, nPaidoff has \<0.1% missing values hence, the impact on the dataset size will be minimal since the remaining 99.9% provides sufficient information.

```{r}
df %>% select(clarityFraudId, clearfraudscore) %>% vis_miss(warn_large_data = FALSE)
```

it seems that the missing values from clarity fraud id and clearfraudscore is not align. Meaning this is not because their clarity ID does not exist. However, we can remove the fraud indicators when there isn't any clarity fraud id while the indicator with present fraud id will be imputed using K-Nearest Neighbor.

```{r}
df_impute = df_feature %>% 
  select(!paymentReturnCode) %>%
  bind_cols(df['clarityFraudId']) %>%
  filter(!is.na(clarityFraudId),
         !is.na(nPaidOff)) 

df_impute %>% 
 vis_miss(warn_large_data = FALSE)
```

Fortunately, at this point the dataset is small enough to be deleted. We still have sufficient information with the remaining variable.

```{r}
df_impute = na.omit(df_impute)
vis_miss(df_impute, warn_large_data = FALSE)
```

Alright, now on to the encoding methods...

### 3.2 Encoding Methods

```{r}

categorical_cols <- df_impute %>% 
  select_if(function(x) is.factor(x) || is.character(x)) %>% 
  colnames()

binary_cols <- df_impute %>%
  select_if(is.logical) %>%
  colnames()

numeric_cols <- df_impute %>% 
  select_if(is.numeric) %>% 
  colnames()

setdiff(colnames(df_impute),c(numeric_cols, categorical_cols, binary_cols))
```

#### 3.3 One-Hot Encoding

```{r}
library(fastDummies)
library(data.table)

df_encoded <- dummy_cols(df_impute,
                          select_columns = binary_cols,
                          remove_first_dummy = FALSE,
                          remove_selected_columns = TRUE)



dt_encoded = data.table(df_encoded)
```

## 4. Modelling

## 5. Result