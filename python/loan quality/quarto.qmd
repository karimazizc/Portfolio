---
title: "In-depth loan status prediction and analysis"
author: "Karim Abdul Aziz Chatab"
format: 
  html: 
    code-fold: true
    code-tools: true
    code-summary: "show code"
    toc: true
    theme: cosmo
    code-overflow: wrap
    self-contained: true
editor: visual
execute:
  warning: false
  message: false
  echo: fenced
---

# Predicting Loan Quality

## 1. Introduction

### 1.1 Business Context

This project was given 3 dataset from data.zip file where I, as the data scientist plans to assess the loan repayment quality of the given customer in the dataset.

### 1.2 Required Libraries

```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from missingno import matrix
import missingno
import plotly.express as px
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
import time
```

### 1.3 Dataset Overview

Full information of the dataset is referred from the dictionary. But in short,

::: callout-note
## loan.csv

This file returns 19 columns consist of the customer's loan information along with their ssn and id. Every row represents and accepted loan application.
:::

::: callout-note
## payment.csv

This file returns 9 columns consist of the customer's payment information along with their ACH error codes and loan id.
:::

::: callout-note
## clarity_underwriting_variables.csv

This file returns 54 rows and 10 columns where each row variable determine the underwriting report.
:::

#### *Importing the dataset:*

```{python}
loan = pd.read_csv('data/loan.csv')
payment = pd.read_csv('data/payment.csv')
clarity = pd.read_csv('data/clarity_underwriting_variables.csv')
```

## 2. Data Preprocessing

### 2.1 Exploratory Data Analysis

```{python}
#display loan
print(loan.head().to_string()) 
```

```{python}
#display clarity
print(clarity.head().to_string()) 
```

```{python}
#display payment
print(payment.head().to_string()) 
```

```{python}
print (f"Payment Rows: {len(payment):,}, Loan Rows: {len(loan):,}")
```

#### 2.1.1 Summary Statistic

##### Frequency table for numerical variables

##### Frequency table for categorical variable

```{python}
# summary table for payment.csv
def describe_full(df):
   numeric_stats = df.describe()
   
   categorical_stats = {col: df[col].value_counts() 
                       for col in df.select_dtypes(include=['object', 'bool']).columns}
   
   print("Numeric Statistics:")
   print(numeric_stats)
   print("\nCategorical Counts:")
   for col, counts in categorical_stats.items():
       print(f"\n{col}:")
       print(counts)

describe_full(loan)
```

```{python}
# Creating a frequency table function
def freq(df, column):
   freq = df[column].value_counts()
   valid_perc = df[column].value_counts(normalize=True) * 100
   total_perc = df[column].value_counts(dropna=False, normalize=True) * 100
   
   table = pd.DataFrame({
       'Freq': freq,
       '% Valid': valid_perc,
       '% Valid Cum.': valid_perc.cumsum(),
       '% Total': total_perc,
       '% Total Cum.': total_perc.cumsum()
   }).round(2)
   
   # Add total row
   total_row = pd.DataFrame({
       'Freq': [len(df)],
       '% Valid': [100],
       '% Valid Cum.': [100],
       '% Total': [100],
       '% Total Cum.': [100]
   }, index=['Total'])
   print(f"--------------- `{column}` Frequency Table ---------------")
   return pd.concat([table, total_row])
freq(loan, 'approved')
```

```{python}
freq(loan, 'originated')
```

```{python}
freq(loan, 'loanStatus')
```

```{python}
freq(loan, 'leadType')
```

```{python}
freq(loan, 'leadType')
```

```{python}
nPaidOff_mean = round(loan['nPaidOff'].dropna().mean() * 100, 2)
print(f"Average pay off rate: {nPaidOff_mean}")
```

loan.csv shows a high-risk lending operation due to very low approval where 40,036 loan was approved out of 577,682 it has 6.93% approval rate. Although relatively high APR (median of 590%), only 7.96% Originated from applications. High APRs, low approval rates, low originated rates, and low pay off rate suggesting this is a high-risk lending operation indicate majority are **subprime borrower base with significant credit risk.**

```{python}

loan_numeric = loan[['originallyScheduledPaymentAmount', 'loanAmount', 'apr']]
paymentAmount_max = loan_numeric['originallyScheduledPaymentAmount'].max()
loanAmount_max = loan_numeric['loanAmount'].dropna().max()

fig = go.Figure()

for col, color, name in [
        ('loanAmount', 'salmon', 'Loan Amount'),
        ('originallyScheduledPaymentAmount', 'turquoise', 'Originally Scheduled Payment Amount')
                        ]:
        fig.add_trace(go.Histogram(
                x=loan_numeric[col].dropna(),
                name=name,
                opacity=0.5,
                marker_color=color,
                nbinsx=30
))

for val, color, name in [
        (loanAmount_max, 'salmon', 'Max Loan'),
        (paymentAmount_max, 'turquoise', 'Max Payment')
]:
        fig.add_vline(
                x=val,
                line_dash="dash",
                line_color=color,
                annotation_text=f"${val:,.2f}",
                annotation_position="top"
        )       

        fig.update_layout(
        barmode='overlay',
        xaxis_title='Dollars (USD)',
        showlegend=True,
        legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="left",
                x=1.05
        ),
        margin=dict(r=150)      
)

fig.show()
```

Assuming this is in USD (since all the states mentioned are from United States). With loan amounts generally being higher than scheduled payments. The dashed line indicate the maximum for loan amount and payment amount. Maximum loan is \$5,000 while maximum original payment shows the 4x multiplier from interest and fees resulting in \$19,963.63 payment amount.

```{python}
freq(loan, 'payFrequency')
```

Majority of 54.94% or 316,654 loans were paid biweekly (B) where least common were paid semi-monthly (S). However, there are mising data in this variable. lets investigate!

```{python}
# This might be used later, hence i create this function 
def view_missing_frequency(df, column):
   missing_freq = df[df[column].isna()].head()
   print(missing_freq.to_string())
   
view_missing_frequency(loan, 'payFrequency')
```

Apparently, the first 5 `approved` variable is false, lets call all the unique values in payfrequency.

```{python}
unique_values = loan[loan['payFrequency'].isna()]['approved'].unique()
print(f"All approved values after filtering pay frequency: {unique_values}")
```

So it seems all NA from pay frequency is a result of unapproved loan.

##### Summary Statistic of payment.csv

```{python}
describe_full(payment)
```

```{python}
freq(payment, 'isCollection')
```

```{python}
freq(payment,'paymentReturnCode')
```

```{python}
freq(payment,'paymentReturnCode')
```

```{python}
freq(payment,'paymentStatus')
```

It seems unusual how, principal, fees and payment amount has a negative value. Although, this could be because of data entry errors, refunds or reversals and system errors in calculation.

```{python}

fees_max = payment['fees'].max()
payamount_max = payment['paymentAmount'].max()
principal_max = payment['principal'].max()

fig = go.Figure()

for col, color in [('principal', 'lightblue'), 
                    ('paymentAmount', 'turquoise'),
                    ('fees', 'salmon')]:
    fig.add_trace(go.Histogram(
        x=payment[col],
        name=col,
        opacity=0.5,
        marker_color=color,
        nbinsx=30
    ))

# Add vertical lines
for val, color, name in [(fees_max, 'salmon', 'Max Fees'), 
                        (payamount_max, 'turquoise', 'Max Payment'),
                        (principal_max, 'lightblue', 'Max Principal')]:
    fig.add_vline(x=val, 
                line_dash="dash",
                line_color=color,
                annotation_text=f"${val:,.2f}",
                annotation_position="top")

fig.update_layout(
    barmode='overlay',
    title='Payment Distributions',
    xaxis_title='Amount',
    showlegend=True,
    legend=dict(
        yanchor="top",
        y=0.99,
        xanchor="left",
        x=1.05
    ),
    margin=dict(r=150)
)

fig.show()
```

```{python}
def plot_installments(df):
   installments = df.groupby('loanId').size().reset_index(name='count')
   
   fig = go.Figure()
   
   # Add histogram
   fig.add_trace(go.Histogram(
       x=installments['count'],
       nbinsx=30,
       marker_color='lightblue'
   ))

   max_val = df['installmentIndex'].max()
   median_val = df['installmentIndex'].median()
   
   for val, text in [(max_val, 'Maximum'), (median_val, 'Median')]:
       fig.add_vline(
           x=val,
           line_dash="dash",
           annotation_text=f"{text}: {val:.0f}",
           annotation_position="top"
       )

   fig.update_layout(
       title='Distribution of Total Installments per Loan',
       xaxis_title='Number of Installments',
       showlegend=False
   )

   # Print summary statistics
   summary = df['installmentIndex'].describe()
   print("\nInstallment Index Summary:")
   print(summary)
   
   return fig
```

The maximum installment is 105 installments, but it's rare - the distribution is heavily right-skewed, suggesting most borrowers have shorter-term repayment schedules where most loans have less than 25 installments payments. The average installments per loan is repaid per 10 installment while the median is 9.

```{python}
def plot_trends(df, min_installments=60):
   # Get loans with installments above threshold
   top_loans = (df.groupby('loanId')['installmentIndex']
               .max()
               .reset_index()
               .query(f'installmentIndex > {min_installments}'))
   
   # Filter payments for those loans
   filtered_payments = df[df['loanId'].isin(top_loans['loanId'])]
   
   fig = px.line(filtered_payments, 
                 x='paymentDate', 
                 y='fees',
                 color='loanId',
                 title=f'Installments >= {min_installments}')
   
   fig.update_layout(showlegend=False)
   
   return fig

plot_trends(payment,50)
```

The graph tracks fee patterns for loans with 60 or more installments from this dataset. Fees generally decrease over time from \$50-150 initial amounts, but show sharp spikes likely representing late payment penalties, collection charges, or loan restructuring events. Most loans follow similar declining patterns despite varying terms indicating the end of their loan term.

#### 2.1.2 Missing Values

```{python}
# missing values for loan.csv
def missing_values(df):
   missingno.matrix(df)
   plt.title('Missing Values in Dataset')
   plt.show()
   
   missing_pct = df.isnull().sum() * 100 / len(df)
   return missing_pct.sort_values(ascending=False)

missing_values(loan)
```

```{python}
missing_values(clarity)

```

```{python}
missing_values(payment)
```

Unfortunately 4 columns in the dataset have more missing values than not. Before we explore further, we need to understand the pattern of these NA values. Firstly for payment.csv, majority of the missing values lies on the **paymentReturnCode.**

```{python}
def print_distinct(df,column):
    unique_codes = df[column].unique()
    chunks = [unique_codes[i:i+5] for i in range(0, len(unique_codes), 5)]

    print(f"-------------- {column} --------------")
    for chunk in chunks:
        print("|", " | ".join(str(code) for code in chunk),"|")
    
print_distinct(payment, 'paymentReturnCode')
```

```{python}
print_distinct(payment,'paymentStatus')
```

```{python}
def count_codes(payment):
   for status in payment['paymentStatus'].unique():
       count = payment[
           (payment['paymentStatus'] == status) & 
           (payment['paymentReturnCode'].notna())
       ].shape[0]
       print(f"{status}: {count}")


count_codes(payment)
```

Note that, payment return code only occurs when the paymentStatus is rejected, checked, cancelled, or returned. Resulting in a code which explains the reject. This is a clear example of MAR (Missing At Random). We can fully explain why the data is missing based on paymentStatus. The same goes to originatedDate and fpStatus. originatedDate only exist if the loan has been originated and fpStatus returns NA if no ACH attempt has been made yet according to the data dictionary.

```{python}

# Count unique loan IDs
unique_payment_loans = payment['loanId'].nunique()
unique_loan_loans = loan['loanId'].nunique()
n = 10
# Get top 5 loan frequencies
payment_counts = (payment['loanId'].value_counts()
                .reset_index()
                .rename(columns={'index': 'loanId', 'loanId': 'count'})
                .head(n))

loan_counts = (loan['loanId'].value_counts()
                .reset_index()
                .rename(columns={'index': 'loanId', 'loanId': 'count'})
                .head(n))
   
print(f"Top 5 loan frequencies in payment data:\n{payment_counts}\n")
print(f"Top 5 loan frequencies in loan data:\n{loan_counts}\n")
print(f"Unique loanId in payment.csv: {unique_payment_loans:,} out of {len(payment):,}")
print(f"Unique loanId in loan.csv: {unique_loan_loans:,} out of {len(loan):,}")
```

payment and loan have different amount of rows, notice that payment's loanID, suggesting that each row does not represent a unique customer. When we try to fit in to the machine learning model, we can't just summarize either, since installments frequency and other ordinal variable might be a useful feature for our prediction. After joining this data set, we need to seperate those rows to be longer where each row represents one loanId. **Due to this we plan to use an encoding methods** later to fit in the model.

Additionally, it is quite strange that the loanId from loan.csv has different order of values, this might not be loanId at all.

```{python}

mask = loan['loanId'].notna() & (~loan['loanId'].astype(str).str.startswith('LL-I-'))
different_ids = loan[mask]

different_ids[['loanId', 'originated', 'loanStatus', 'approved']]


```

These id are a result of loanStatus being paid off, withdrawn, pending application, void and external collection.

#### 2.1.3 Joining data

```{python}

df_merged = (payment.merge(loan, on='loanId', how='left')
                  .merge(clarity, 
                        left_on='clarityFraudId', 
                        right_on='underwritingid', 
                        how='left'))
   
# Display first 5 rows and summary
pd.set_option('display.max_columns', None)
print("------------------ Joined Data Frame ------------------")
print("Joined dataset of loan.csv, payment.csv and clarity_underwriting_variables.csv")
print(f"Total Rows: {len(df_merged):,}, Total Columns: {len(df_merged.columns)}")

df_merged.head()
```

Before we can explore further on this dataset, we want to know if there are any missing values.

Additionally, we want to assess whether this join is successful or not

```{python}

def check_missing():
   # Calculate missing percentages
   missing_pct = (df_merged.isnull().mean() * 100).reset_index()
   missing_pct.columns = ['variable', 'percent_missing']
   
   # Calculate missing counts
   missing_counts = df_merged.isnull().sum().reset_index()
   missing_counts.columns = ['variable', 'n_missing']
   
   # Combine and calculate not missing
   missing_stats = missing_counts.merge(missing_pct, on='variable')
   missing_stats['not_missing'] = len(df_merged) - missing_stats['n_missing']
   
   # Filter and sort
   missing_stats = missing_stats[missing_stats['percent_missing'] > 0].sort_values(
       'percent_missing', ascending=False)
   
   missing_stats['percent_missing'] = missing_stats['percent_missing'].round(5)
   
   return missing_stats

check_missing()
```

it seems the initial fraud screening has 19-20% missing values while clear fraud identity verification has missing rates of 95-97% implying this is a result of selective verfication process based on risk assessment needs. Due to this we plan to not include all the variables for the model as the high NA values might add white noise to the model. Although, the clarity underwriting values that we'll prioritize is:

-   clearFraudScore (measure overall fraud risk)

-   totalnumberoffraudindicators (aggregate fraud signals)

-   clearfraudinquiry (inquiries of n-th times ago)

-   ssnnamematch (verify identity)

-   nameaddressmatch (address verification)

-   overallmatchresult (combined verficiation score)

-   creditestablishedbeforeage18 (potential fraud)

-   currentaddressreportedbynewtradeonly (suspicious signal)

-   morethan3inquiriesinthelast30days (shows desperation)

-   inputssninvalid (basic fraud check)

### 2.2 Determining the dependent variable.

```{python}
print_distinct(payment,'paymentStatus')
```

After the exploratory data analysis, **PaymentStatus** would be the ideal dependent variable for predicting loan risk. This because it captures the complete process of loan performance. Payment status have no NA values and updates throughout the loan term. Additionally, paymentstatus reveals the 9 indicators of loan progression from "Checked" to "Complete". This variable aligns perfectly with the business context where it is a measure of the applicant's behaviour and the loan's health.

## 3. Feature Engineering

Our goal here is to reduce dimensionality and create a better predictive power. realistically, we'd implement the use of a white box model. That is because we plan to create a model that can still be interpret rather than being highly accurate. To compensate on predictive power, we want the model to be explainable due to financial regulations prefer our fraud prediction to be transparent for fair lending practices, regulatory compliance, model validation and customer explanation requirements.

However, for this analysis purpose we will try to explore both black and white box model as our comparison to see how much predictive power have we trade off for interpretability.

### 3.1 Selecting Useful Inputs

For the maching learning model to work, we wanted to delete any redundant variables that will not be useful for prediction. This include, ID , highly correlated variables (principal and fees with payment amount) and dates. We also wanted to remove state as that might introduce regional bias. Although, we plan to keep the loanID to aggregate the columns later.

```{python}

# Select initial features
selected_cols = [
    'loanId', 'paymentStatus', 'installmentIndex', 'isCollection',
    'principal', 'fees', 'paymentReturnCode', 'payFrequency', 'apr',
    'isFunded', 'approved', 'loanAmount', 'originallyScheduledPaymentAmount',
    'nPaidOff', 'originated', 'loanStatus', 'leadType', 'fpStatus',
    'clearfraudscore'
]

# Clarity columns -> underwriting variables that we belive to be useful
fraud_patterns = [
    'clearfraudinquiry', 'creditestablishedbeforeage18', 'ssnnamematch',
    'currentaddressreportedbynewtradeonly', 'morethan3inquiriesinthelast30days',
    'overallmatchresult', 'totalnumberoffraudindicators', 'inputssninvalid',
    'nameaddressmatch'
]

for pattern in fraud_patterns:
    pattern_cols = [col for col in df_merged.columns if pattern in col.lower()]
    selected_cols.extend(pattern_cols)

df_feature = df_merged[selected_cols].copy()

# Rename columns by taking last part after dot
rename_dict = {col: col.split('.')[-1] for col in df_feature.columns if '.' in col}
df_feature = df_feature.rename(columns=rename_dict)

print(f"Columns: {len(df_feature.columns)}, Rows: {len(df_feature):,}")
df_feature.head()
```

```{python}

def plot_corr():
   df_num = df_feature.select_dtypes(include=['float64', 'int64']).dropna() 
   df_num.columns = [col[:10] for col in df_num.columns]
   corr = df_num.corr()
   
   plt.figure(figsize=(12, 10))
   sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
   plt.tight_layout()
   return plt.show()
plot_corr()
```

This correlation matrix present correlation challenges for the model. High multicollineaity between time variables might need to be aggregated to prevent redundancy. Creating ratio features would probably be efficient. But for now, let's just select one to simplify this model, we'd select fifteendaysago. For the installment index, as discussed previously in data preprocessing stage we plan to implement an encoding methods to aggregate the data.

### 3.2 Missing Values by removal

```{python}
missing_values(df_feature)
```

If we want to filter the data we need to make sure the missing data is Missing Completely At Random (MCAR) so it wont create selection bias, hoping this missingness doesn't correlate with important outcomes. Additionally the payment return code has 95% missing values, it is better to completely drop them. The fraud indicators likely to be Missing At Random (MAR) or Missing Not At Random (MNAR) as the missing visualization shows consistent pattern. to double check, this data is a result of joining three tables. Hence, let's visualize the clarity fraud id with clearfraudscore on the previous joined dataset. However, nPaidoff has \<0.1% missing values hence, the impact on the dataset size will be minimal since the remaining 99.9% provides sufficient information.

```{python}
missing_values(df_merged[['clarityFraudId', 'clearfraudscore']])
```

it seems that the missing values from clarity fraud id and clearfraudscore is not align. Meaning this is not because their clarity ID does not exist. However, we can remove the fraud indicators when there isn't any clarity fraud id while the indicator with present fraud id will be imputed using K-Nearest Neighbor.

```{python}

imputed = df_feature.copy()
imputed = imputed.drop(columns=['paymentReturnCode'])

# Filter out null values in key columns
df_impute = imputed[imputed['nPaidOff'].notna()]

missing_values(df_impute)
```

Fortunately, at this point the dataset is small enough to be deleted. We still have sufficient information with the remaining variable. Otherwise, we do not want the imputated data to mess with the model.

```{python}
df_impute = df_impute.dropna()
columns_to_drop = [col for col in df_impute.columns if col.endswith('ago') and col != 'fifteendaysago']
df_impute = df_impute.drop(columns=columns_to_drop)
```

```{python}
df_impute.head()
```

Alright, now on to the encoding methods...

### 3.2 Encoding Methods

Now before we continue further, there are two ways this could work. If we retain all the long information from payment.csv which allows the same loanId to be present untill all installment index, Or we derive them sequentially and create a weighted risk level assessment. From here lets create the function we need and then we try to do both.

#### I/0 Binary Encoding

```{python}
categorical = ['paymentStatus', 'payFrequency', 'loanStatus', 'leadType', 'fpStatus', 'ssnnamematch', 'overallmatchresult', 'nameaddressmatch']

binary = ['isCollection', 'approved', 'originated', 'creditestablishedbeforeage18',
                'currentaddressreportedbynewtradeonly', 'morethan3inquiriesinthelast30days',
                'inputssninvalid']

# One-hot encode binary columns
df_encoded = pd.get_dummies(df_impute, columns=binary, drop_first=False)

bool_columns = df_encoded.select_dtypes(include=['bool']).columns

# Convert boolean to 0/1 integers
for col in bool_columns:
   df_encoded[col] = df_encoded[col].astype(int)

df_encoded
```

```{python}
binary_list = []
for x in binary:
    binary_list.append(f"{x}_True")
    binary_list.append(f"{x}_False")
print(binary_list)
```

#### Label encoding

```{python}
# Print your categorical columns
print("Categorical columns:", categorical)

# Print dataframe columns 
print("\nDataframe columns:", df_encoded.columns.tolist())
```

```{python}
def encode_categories(df):
    
    df_encoded = df.copy()

    encoders = {}
    
    for col in categorical:
        encoders[col] = LabelEncoder()
        df_encoded[col] = encoders[col].fit_transform(df[col].fillna('Missing'))
    
    return df_encoded, encoders


df_en, encoders = encode_categories(df_encoded)

for i in categorical[1:]:
    print_distinct(df_en, i)
```

```{python}
for label, value in enumerate(encoders['paymentStatus'].classes_):
    print(f"{value}: {label}")
```

Great! it works. Our last concern now is how do we aggregate the installment index and the behaviour on during those installments. If we were to sequentally approache the feature to add on like installment_1, payment_1, principal_1 then it would resulted in a missing value. Before that, we need to modify this label encoder.

```{python}
def encode_loan_categories(df):
    df_encoded = df.copy()
    encoders = {}

    risk_order = {
        'Checked': 0,
        'Pending': 1, 
        'Rejected Awaiting Retry': 2,
        'Skipped': 3,
        'Cancelled': 4,
        'Rejected': 5,
        'None': 6
    }
    df_encoded['paymentStatus'] = df['paymentStatus'].map(risk_order)

    categorical = df.select_dtypes(include=['object']).columns
    for col in [c for c in categorical if c != 'paymentStatus']:
        encoders[col] = LabelEncoder()
        df_encoded[col] = encoders[col].fit_transform(df[col].fillna('Missing'))
    
    return df_encoded, encoders

# Apply encoding
df_en, encoders = encode_loan_categories(df_encoded)
```

#### **Sequential Wide data frame transformation**

```{python}
def sequential_feature(df, max_installments):
    df_sorted = df.sort_values(['loanId', 'installmentIndex'])
    features = {}
    
    
    defaults = {
        'paymentStatus': 6,
        'installmentIndex': 0,
        'principal': 0.0,
        'fees': 0.0,
        'isCollection': 0  
    }
    
    for loan_id, loan_data in df_sorted.groupby('loanId'):
        loan_features = {}
        
        for i in range(1, max_installments + 1):
            loan_features.update({
                f'payment{i}_status': defaults['paymentStatus'],
                f'payment{i}_index': defaults['installmentIndex'],
                f'payment{i}_principal': defaults['principal'],
                f'payment{i}_fees': defaults['fees'],
                f'payment{i}_isCollection': defaults['isCollection']
            })
        
        # Fill actual values
        for i, payment in enumerate(loan_data.itertuples(), 1):
            if i <= max_installments:
                loan_features[f'payment{i}_status'] = payment.paymentStatus
                loan_features[f'payment{i}_index'] = payment.installmentIndex
                loan_features[f'payment{i}_principal'] = payment.principal
                loan_features[f'payment{i}_fees'] = payment.fees
                # Check which column exists
                if hasattr(payment, 'isCollection'):
                    loan_features[f'payment{i}_isCollection'] = payment.isCollection
                elif hasattr(payment, 'isCollection_True'):
                    loan_features[f'payment{i}_isCollection'] = payment.isCollection_True
                elif hasattr(payment, 'isCollection_False'):
                    loan_features[f'payment{i}_isCollection'] = not payment.isCollection_False
        
        features[loan_id] = loan_features
    
    return pd.DataFrame.from_dict(features, orient='index')

df_wide = sequential_feature(df_en, max(df_en['installmentIndex']))
```

```{python}
for x in range(10):
    print_distinct(df_wide, f'payment{x+1}_status')
```

#### **concurrent loanId long data frame**

```{python}
df_1, encoder_1= encode_categories(df_impute)
df_long = df_1.iloc[:, 1:]
df_long
```

## 4. Modelling

### 4.1 Story of two data

We've assembled two types of dataset for this model. To put it simply, one long and one wide.:

-   long dataset `df_long` : installmentIndex remains the same. For each installmentindex, the loan id is the same until it moves on the next installmentIndex
-   wide dataset `df_wide` : Each row is a unique loanId. installmentIndex is sequentially added along with supplementary information such as paymentStatus, and the rest.

df_long use case: this dataset will be fitted into a machine learning model attempting to predict payment status and interpret result df_wide use case: to develop our own algorithm that can categorize specific borrowers of their risk levels, aligning with our business context

### 4.2 Built-in Algorithm

```{python}
def weighted_risk_score(df):
    status_columns = [col for col in df.columns if 'status' in col]
    
    # Define risk weights that increase with severity
    risk_weights = {
        0: 0.0,   # Completed - Lowest risk 
        1: 0.2,     
        2: 0.5,     
        3: 0.7,    
        4: 0.9,     
        5: 1.0      # Rejected - Maximum risk
    }
    
    def time_weight(payment_number):
        return 1 / (1 + 0.1 * payment_number)  
    

    def calculate_loan_risk(row):
        total_weight = 0
        risk_score = 0
        
        for i, col in enumerate(status_columns):
            status_value = row[col]
            risk_value = risk_weights.get(status_value, 0)
            time_value = time_weight(i)
            
            risk_score += risk_value * time_value
            total_weight += time_value
            
        return (risk_score / total_weight) if total_weight > 0 else 0
    df['risk_score'] = df.apply(calculate_loan_risk, axis=1)
    # Categorize risk levels
    df['risk_category'] = pd.cut(df['risk_score'], 
                                bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],
                                labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
    
    return df

df_result = weighted_risk_score(df_wide)
```

```{python}
loan_ids = pd.DataFrame(df_impute['loanId'].unique(), columns=['loanId'])

df_risk = pd.concat([loan_ids, df_result[['risk_score', 'risk_category']]], axis=1)

df_risk
```

Alright this is just a risk weightage algorithm I thought of as an intution. Therefore, no machine learning model is used.

The algorithm uses two main components:

1.  Risk Weights: We assign increasing weights from 0 to 1 based on payment status severity. It's like a grading scale where:

-   0 (Checked) = A+ borrower, always pays on time
-   0.2 (Pending) = B student, mostly reliable
-   0.5 (Rejected Awaiting Retry) = C borrower, showing concerning signs
-   0.7 (Cancelled) = D borrower, significant issues -\> probably financial issues or personal.
-   0.9 (Skipped) = F borrower, **major** red flags
-   1.0 (Rejected) = Complete failure to meet requirements

2.  Time Decay Function: We use an exponential decay formula: 1/(1 + 0.1 \* payment_number) Think of it like your memory - recent events are clearer than distant ones, but early payments carry special weight in predicting behavior.

The final risk score combines these factors: risk_score = Σ(risk_weight \* time_weight) / Σ(time_weights)

I divide them by total weights to normalize this approach so we can ensure scores ranges between 0-1 making them easy to interpret and categorize into risk levels. This draws inspiration from FICO scores where they look at payment history with higher weights on recent activities.

Therefore, lets say loanId LL-I-00202645 has a risk score of 0.339224 on which we categorize it into low category. There are some issue with this. since some category is Nan, but this is up for discussions with other data scientist so we can develop our own model based on the relevant business context and situation.

### 4.3 Traditional Machine Learning model

#### **4.3.1 Data Partitioning**

```{python}
X = df_long.drop('paymentStatus', axis=1)
y = df_long['paymentStatus']
print(f"Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

#### 4.3.2 Model Building

```{python}
class_weights = {0: 1, 1: 1, 2: 5, 3: 5, 4: 10, 5: 10, 6: 15}
models = {
'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
'Logistic Regression': LogisticRegression(class_weight=class_weights, max_iter=1000),
'Decision Tree': DecisionTreeClassifier(random_state=42),
'KNN': KNeighborsClassifier(n_neighbors=5),
'Naive Bayes': GaussianNB()
}
results = {}
def machine_learning(X_train_scaled,y_train, model):
    
    print(f"fitting models...\n")


    print(f"Stage 1: fitting {model}")
    start = time.time()
    model.fit(X_train_scaled, y_train)

    print(f"Stage 2: predicting {model}")
    y_pred = model.predict(X_test_scaled)

    print(f"Stage 3: collecting accuracy {model}")
    accuracy = accuracy_score(y_test, y_pred)
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)


    results[model] = {
    'Accuracy': accuracy,
    'CV Mean': cv_scores.mean(),
    'CV Std': cv_scores.std(),
    'Model': model,
    'Classification Report': classification_report(y_test, y_pred)}

    print(f"Test Accuracy: {results[model]['Accuracy']:.4f}")
    print(f"CV Mean Accuracy: {results[model]['CV Mean']:.4f} (+/- {results[model]['CV Std']*2:.4f})")
    print("\nClassification Report:")
    print(results[model]['Classification Report'])
    end = time.time()
    print(f"{model} took {end-start/60} minutes\n")
    print("---------------------------------------------\n")

    return results, scaler
```

#### **4.3.3 Random Forest**

```{python}
results_rf, scaler_rf = machine_learning(X_train_scaled,y_train, model=models['Random Forest'])
```

Random Forest results show 87.78% test accuracy with stable cross-validation (87.05% ± 0.15%). Strong performance for common statuses (Checked/Pending), weak for rare cases (Rejected/Skipped). Class imbalance evident from support numbers.

#### **4.3.4 Logistic Regression**

```{python}
results_lr, scaler_lr = machine_learning(X_train_scaled,y_train, models['Logistic Regression'])
```

Logistic Regression achieves 67.28% accuracy (lower than random forest) with good detection of low-risk loans but critically fails on high-risk categories despite class weighting. Poor recall on risky loans makes it unsuitable for loan assessment.

#### **4.3.5 Decision Tree**

```{python}
results_dt, scaler_dt = machine_learning(X_train_scaled,y_train, models['Decision Tree'])
```

The Decision Tree classifier achieves 80% accuracy, performing well on common payment statuses but struggling with rare cases. Its cross-validation stability at 79.18% demonstrates consistent but imperfect predictive ability.

#### **4.3.5 K Nearest Neighbor (KNN)**

```{python}
results_dt, scaler_dt = machine_learning(X_train_scaled,y_train, models['KNN'])
```

85.15% accuracy, slightly lower than Random Forest (87.78%) but better than Logistic Regression (67.28%) and Decision Tree (79.97%). Strong with common payment statuses but near-zero performance on rare cases. Cross-validation shows stable performance at 83.62%.

#### **4.3.6 Naive Bayes**

```{python}
results_dt, scaler_dt = machine_learning(X_train_scaled,y_train, models['Naive Bayes'])
```

Naive Bayes performs extremely poorly at 10.92% accuracy - significantly worse than all other models (RF: 87.78%, KNN: 85.15%, DT: 79.97%, LR: 72.92%). Shows unusual behavior by catching rare cases but missing common ones. **NOT SUITABLE** for this payment status prediction task.

#### **4.3.7 Champion model and comparison**

Random Forest emerges as the champion model with 87.78% accuracy, demonstrating superior performance across payment status predictions. Its strength lies in handling both common statuses (Checked/Pending: \>90% precision) and maintaining moderate performance on rare cases.

Model comparison by accuracy: 1. Random Forest: 87.78% 2. KNN: 85.15% 3. Decision Tree: 79.97% 4. Logistic Regression: 72.92% 5. Naive Bayes: 10.92%

While Random Forest excels in accuracy, KNN performs well but struggles with rare cases. Logistic Regression handles only majority classes effectively. Naive Bayes proves unsuitable for this payment prediction task.

**All models face challenges with class imbalance,** particularly for statuses like Rejected and Skipped. For production implementation, Random Forest's balance of accuracy and stability makes it the recommended choice for prediction. For interpretation However, decision tree will be best.

#### **4.3.8 Interpreting decision tree**

```{python}
from sklearn.inspection import partial_dependence, PartialDependenceDisplay
import matplotlib.pyplot as plt

features_to_plot = [0, 1, 2, 3,4,5]  
feature_names = ['installmentIndex', 'isCollection', 'fees',
                 'fpStatus', 'clearfraudscore','payFrequency' ]

display = PartialDependenceDisplay.from_estimator(
  models['Decision Tree'].fit(X_train_scaled, y_train),
  X_train_scaled,
  features_to_plot,
  target=0,  # Add target class
  feature_names=feature_names
)
plt.tight_layout()
plt.show()
```

#### **4.3.9 Decision Tree interpretation**

According to the decision tree model, The partial dependence plots reveal that installment index is the strongest predictor, showing loan risk increases significantly in early payments before stabilizing. Fraud scores have a gradual negative impact, suggesting higher scores correlate with lower risk. Surprisingly, fees is the most stable and Collection status shows minimal influence. Additionally, first payment status maintains consistent impact across categories.

Interesting note is that pay frequency's spikes might indicate that borrowers who choose unusual payment schedules could be facing underlying financial challenges. It's like how someone asking for non-standard payment arrangements might be showing signs of financial stress.

## 5. Model Limitation

as expected, the machine learning model for random forest, decision tree and knn is predict far superior than Logistic regression and Naive Bayes

#### **Limitation:**

-   As this is just a simple model before we scale up, our model will eventually become more sophisticated.
-   ALL models struggle with minority classes
-   Data is heavily imbalanced, which might explain why they all performed poorly on minority classess. As algorithms naturally bias toward majority classes to maximize overall accuracy.
-   Models is not ready for professional deployment, need to perform hyperparameter tuning

In our loan payment analysis, with common statuses like 'Checked' having nearly 40 times more examples than 'Skipped' or 'Rejected', models couldn't learn reliable patterns for rare cases. This imbalance particularly affected simpler algorithms like Logistic Regression, which failed completely on minority classes. Even the more sophisticated Random Forest, while performing better overall, still showed significant weakness in predicting these critical but uncommon payment statuses.

#### **Potential Solutions:**

-   Oversampling methods
-   Class weights
-   Collecting more data for rare cases
-   Better feature engineering

#### **Fitting issues:**

Random Forest (87.78% test, 87.05% CV): - Slight overfitting, but its not an issue - Complex model capturing noise

Logistic Regression (67.28% test, 67.18% CV): - Balanced but scored very low - Too simple for complex patterns

KNN (85.15% test, 83.62% CV): - Moderate overfitting - Sensitive to training data

Decision Tree (79.97% test, 79.18% CV): - Slight overfitting, but its not an issue - Missing some key patterns

All models particularly struggle with minority classes.